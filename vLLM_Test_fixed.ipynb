{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1hj-luPK1DORqN_skUMz8TPyEIR4RgzSh",
      "authorship_tag": "ABX9TyOEe9tBhqhdThqG5W7oC300"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# GPU 확인\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMkab14JSulI",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1759378004206,
          "user_tz": -540,
          "elapsed": 163,
          "user": {
            "displayName": "Paul Youngsub Lim",
            "userId": "17082658625537161560"
          }
        },
        "outputId": "0a161fc9-a9c4-434a-f8ab-8c2e6c300bb3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Oct  2 04:06:44 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   56C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['VLLM_USE_V1'] = '0'"
      ],
      "metadata": {
        "id": "uGo4SMSkSuiZ",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1759378004210,
          "user_tz": -540,
          "elapsed": 1,
          "user": {
            "displayName": "Paul Youngsub Lim",
            "userId": "17082658625537161560"
          }
        }
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vLLM 설치\n",
        "!pip install vllm -q"
      ],
      "metadata": {
        "id": "T9uxrHY3SufN",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1759378013160,
          "user_tz": -540,
          "elapsed": 8948,
          "user": {
            "displayName": "Paul Youngsub Lim",
            "userId": "17082658625537161560"
          }
        }
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 메모리 정리\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yb6C-wDQSub1",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1759378017656,
          "user_tz": -540,
          "elapsed": 4494,
          "user": {
            "displayName": "Paul Youngsub Lim",
            "userId": "17082658625537161560"
          }
        },
        "outputId": "49b405b2-d008-4940-8035-c3799fa6a648"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 로딩\n",
        "\n",
        "from vllm import LLM, SamplingParams\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import time\n",
        "\n",
        "model_name = \"facebook/opt-125m\"\n",
        "print(f\"\\n[모델 로딩 시작] {model_name}\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "print(\"\\n1. vLLM 모델 로드... \")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "start_vllm_load = time.time()\n",
        "llm = LLM(model=model_name)\n",
        "vllm_load_time = time.time() - start_vllm_load\n",
        "\n",
        "print(f\"   vLLM 로드 시간: {vllm_load_time:.2f}초\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "print(\"\\n2. HuggingFace 모델 로드 중...\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "start_hf_load = time.time()\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "hf_model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\")\n",
        "hf_load_time = time.time() - start_hf_load\n",
        "\n",
        "print(f\"   HuggingFace 로드 시간: {hf_load_time:.2f}초\")\n",
        "print(\"-\" * 80)\n",
        "print(f\">> vLLM의 초기 로드 시간이 HuggingFace보다 긴 것이 일반적:  KV 캐시 블록 풀 메모리 사전 할당, CUDA 커널들을 사전 컴파일하고 최적화, 배치 처리 준비\")\n",
        "print(f\">> HuggingFace는 모델만 로드하고 추론 시점에 필요한 만큼만 메모리를 할당하여 초기 로딩 속도가 더 빠름\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 927,
          "referenced_widgets": [
            "396600077253475ca53bc132dffe92d3",
            "8900027899cc49babdb7cb623b19a0e5",
            "812339c1a00a471cbdaa24a450e8a5f5",
            "709a2f416bb34c79b1b14c24e77fc4eb",
            "e094ab61351a4de58c2889f986b2b4ab",
            "1818fea926d545f7969279bfd81c7452",
            "57e3e39209ab47c8b86be19024c25651",
            "9f398698458545009a401b060d592ecb",
            "3fd340b8a6164de19c78fb8f220f2610",
            "59515fcbcccc4e7bb3a2b944c08e25f4",
            "a86f4fb3118341a2904118de10180339",
            "13c797febc3f46b8b903fac647961c27",
            "b5a470f72e2a4a93b789d7b86300265e",
            "952936c9f47a48aea924c346b1d7d138",
            "d80ce4d14879407a8384afff66e68fcf",
            "180d2ef8f0b24ecb80800b7b5e21fccf",
            "1ea1754e7f984eaa955d8b7c5fd76f3a",
            "ddc08e3f4d72455794b8f3dda4505fc5",
            "b214c5ef70b8458298ee3c4ea950bfaa",
            "afe45cdff81948f3a6e253c5ef5cb84c",
            "65308939d5be42e0a78fa88b75928702",
            "2c4e1b558a674c2baac5345b4bd6ece7"
          ]
        },
        "id": "OV66B41zgNgE",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1759378133514,
          "user_tz": -540,
          "elapsed": 115857,
          "user": {
            "displayName": "Paul Youngsub Lim",
            "userId": "17082658625537161560"
          }
        },
        "outputId": "2339c7d3-869d-4a2f-e1c5-36beefc03538"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 10-02 04:07:21 [__init__.py:216] Automatically detected platform cuda.\n",
            "\n",
            "[모델 로딩 시작] facebook/opt-125m\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "1. vLLM 모델 로드... \n",
            "--------------------------------------------------------------------------------\n",
            "INFO 10-02 04:07:25 [utils.py:328] non-default args: {'disable_log_stats': True, 'model': 'facebook/opt-125m'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 10-02 04:07:51 [__init__.py:742] Resolved architecture: OPTForCausalLM\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 10-02 04:07:51 [__init__.py:1815] Using max model len 2048\n",
            "INFO 10-02 04:07:55 [llm_engine.py:221] Initializing a V0 LLM engine (v0.10.2) with config: model='facebook/opt-125m', speculative_config=None, tokenizer='facebook/opt-125m', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=facebook/opt-125m, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":0,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":256,\"local_cache_dir\":null}, use_cached_outputs=False, \n",
            "INFO 10-02 04:07:57 [cuda.py:408] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
            "INFO 10-02 04:07:57 [cuda.py:453] Using XFormers backend.\n",
            "INFO 10-02 04:07:58 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "INFO 10-02 04:07:58 [model_runner.py:1051] Starting to load model facebook/opt-125m...\n",
            "INFO 10-02 04:07:58 [weight_utils.py:348] Using model weights format ['*.safetensors', '*.bin', '*.pt']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading pt checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "396600077253475ca53bc132dffe92d3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 10-02 04:08:00 [default_loader.py:268] Loading weights took 1.19 seconds\n",
            "INFO 10-02 04:08:01 [model_runner.py:1083] Model loading took 0.2389 GiB and 1.672711 seconds\n",
            "INFO 10-02 04:08:02 [worker.py:290] Memory profiling takes 1.05 seconds\n",
            "INFO 10-02 04:08:02 [worker.py:290] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.90) = 13.27GiB\n",
            "INFO 10-02 04:08:02 [worker.py:290] model weights take 0.24GiB; non_torch_memory takes 0.03GiB; PyTorch activation peak memory takes 0.47GiB; the rest of the memory reserved for KV Cache is 12.53GiB.\n",
            "INFO 10-02 04:08:03 [executor_base.py:114] # cuda blocks: 22813, # CPU blocks: 7281\n",
            "INFO 10-02 04:08:03 [executor_base.py:119] Maximum concurrency for 2048 tokens per request: 178.23x\n",
            "INFO 10-02 04:08:07 [model_runner.py:1355] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "13c797febc3f46b8b903fac647961c27"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 10-02 04:08:52 [model_runner.py:1507] Graph capturing finished in 44 secs, took 0.14 GiB\n",
            "INFO 10-02 04:08:52 [worker.py:467] Free memory on device (14.64/14.74 GiB) on startup. Desired GPU memory utilization is (0.9, 13.27 GiB). Actual usage is 0.24 GiB for weight, 0.47 GiB for peak activation, 0.03 GiB for non-torch memory, and 0.14 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=13152064614` to fit into requested memory, or `--kv-cache-memory=14626958848` to fully utilize gpu memory. Current kv cache memory in use is 13456151654 bytes.\n",
            "INFO 10-02 04:08:52 [llm_engine.py:420] init engine (profile, create kv cache, warmup model) took 50.94 seconds\n",
            "INFO 10-02 04:08:52 [llm.py:295] Supported_tasks: ['generate']\n",
            "INFO 10-02 04:08:52 [__init__.py:36] No IOProcessor plugins requested by the model\n",
            "   vLLM 로드 시간: 86.84초\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "2. HuggingFace 모델 로드 중...\n",
            "--------------------------------------------------------------------------------\n",
            "   HuggingFace 로드 시간: 2.03초\n",
            "--------------------------------------------------------------------------------\n",
            ">> vLLM의 초기 로드 시간이 HuggingFace보다 긴 것이 일반적:  KV 캐시 블록 풀 메모리 사전 할당, CUDA 커널들을 사전 컴파일하고 최적화, 배치 처리 준비\n",
            ">> HuggingFace는 모델만 로드하고 추론 시점에 필요한 만큼만 메모리를 할당하여 초기 로딩 속도가 더 빠름\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"1. 단일 추론 속도 비교\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "prompt = \"The future of AI is\"\n",
        "params = SamplingParams(temperature=0.8, max_tokens=50)\n",
        "\n",
        "# vLLM\n",
        "print(\"\\n[vLLM 추론]\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "start_vllm = time.time()\n",
        "vllm_outputs = llm.generate([prompt], params)\n",
        "vllm_time = time.time() - start_vllm\n",
        "\n",
        "print(f\">> 입력: {prompt}\")\n",
        "print(f\">> 출력: {vllm_outputs[0].outputs[0].text[:100]}...\")\n",
        "print(f\">> 처리 시간: {vllm_time:.3f}초\")\n",
        "\n",
        "\n",
        "# HuggingFace\n",
        "print(\"\\n[HuggingFace 추론]\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "start_hf = time.time()\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "with torch.no_grad():\n",
        "    hf_outputs = hf_model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=50,\n",
        "        temperature=0.8,\n",
        "        do_sample=True\n",
        "    )\n",
        "hf_time = time.time() - start_hf\n",
        "hf_text = tokenizer.decode(hf_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(f\">> 입력: {prompt}\")\n",
        "print(f\">> 출력: {hf_text[len(prompt):len(prompt)+100]}...\")\n",
        "print(f\">> 처리 시간: {hf_time:.3f}초\")\n",
        "\n",
        "print(f\"\\n>> 속도 비교: vLLM이 HuggingFace보다 {hf_time/vllm_time:.1f}배 빠름\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448,
          "referenced_widgets": [
            "4fab319a92214c3ea5121e0771cd78ca",
            "da2a74088af846dcbdf1a46677e917a5",
            "51fdf23ca57540459cdb839252f7ddfa",
            "2dfbfb4131db4137aa1226a6203a0dc0",
            "cd714d34161f4cb4adfe1ceaada56da2",
            "be834f8b994a4b518ebcf45d5385968b",
            "7a38bac15a534994a90707252f399164",
            "a64a556c2137412486e67b9f5a33a242",
            "759033378fa543b69ee298efc0c86e32",
            "6d960ce2c7c64e19b797254c07983268",
            "0eba9bea3b8945f69bdd736cade4dd48",
            "e734de2ab78543c2836ba8dc1b687985",
            "e3eb92e2f17a466eb65f2353e4a0dd27",
            "8d9aa3059a774edfa093dddf0ce380e5",
            "2e7c8fa8e8064bc0b50223a8d0d2ffda",
            "23473175f28a496db79a75c69940a757",
            "8b3418ea93094f03b6b62618b49753c5",
            "c83394c32c52440fa3ace04c6926873d",
            "1e66438ca83e4544b939cbe359a9f8be",
            "2f8bc44b5ada49cab7a5cbd4509bc967",
            "3a5b351f1c6f45b6aba7b5efcc6da6eb",
            "262e27b30ead4481a3925e1e87d3d5ad"
          ]
        },
        "id": "syX5oL4AgUkf",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1759378135617,
          "user_tz": -540,
          "elapsed": 2101,
          "user": {
            "displayName": "Paul Youngsub Lim",
            "userId": "17082658625537161560"
          }
        },
        "outputId": "2b4fa785-3d9e-4035-dd78-4b5cf8e34d4e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. 단일 추론 속도 비교\n",
            "================================================================================\n",
            "\n",
            "[vLLM 추론]\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4fab319a92214c3ea5121e0771cd78ca"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e734de2ab78543c2836ba8dc1b687985"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> 입력: The future of AI is\n",
            ">> 출력:  made by scientists.  If only it were true.\n",
            "I think you mean \"the future...\".\n",
            "The past is not the fu...\n",
            ">> 처리 시간: 0.517초\n",
            "\n",
            "[HuggingFace 추론]\n",
            "--------------------------------------------------------------------------------\n",
            ">> 입력: The future of AI is\n",
            ">> 출력:  in AI and it seems that humans are starting to understand AI better than ever before.\n",
            "\n",
            "One of the m...\n",
            ">> 처리 시간: 1.576초\n",
            "\n",
            ">> 속도 비교: vLLM이 HuggingFace보다 3.0배 빠름\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"2. 배치 처리 속도 비교 (Continuous Batching)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "prompts = [\n",
        "    \"Artificial intelligence can\",\n",
        "    \"Machine learning is\",\n",
        "    \"Deep learning helps\",\n",
        "    \"Neural networks are\",\n",
        "    \"Python programming\"\n",
        "]\n",
        "\n",
        "# vLLM\n",
        "print(f\"\\n[vLLM 배치 추론] {len(prompts)}개 동시 처리\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "start_vllm_batch = time.time()\n",
        "vllm_batch_outputs = llm.generate(prompts, params)\n",
        "vllm_batch_time = time.time() - start_vllm_batch\n",
        "\n",
        "for i, output in enumerate(vllm_batch_outputs[:5]):\n",
        "    print(f\"  [{i+1}] {prompts[i]}\")\n",
        "    print(f\"      -> {output.outputs[0].text[:60]}...\")\n",
        "\n",
        "print(f\"  ... (총 {len(prompts)}개)\")\n",
        "print(f\">> 총 처리 시간: {vllm_batch_time:.3f}초\")\n",
        "print(f\">> 처리량: {len(prompts)/vllm_batch_time:.2f} 요청/초\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# HuggingFace\n",
        "print(f\"\\n[HuggingFace 배치 추론] {len(prompts)}개 순차 처리\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "start_hf_batch = time.time()\n",
        "hf_batch_outputs = []\n",
        "for prompt_item in prompts:\n",
        "    inputs = tokenizer(prompt_item, return_tensors=\"pt\").to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        outputs = hf_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=50,\n",
        "            temperature=0.8,\n",
        "            do_sample=True\n",
        "        )\n",
        "    hf_batch_outputs.append(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "hf_batch_time = time.time() - start_hf_batch\n",
        "\n",
        "for i in range(5):\n",
        "    print(f\"  [{i+1}] {prompts[i]}\")\n",
        "    print(f\"      -> {hf_batch_outputs[i][len(prompts[i]):len(prompts[i])+60]}...\")\n",
        "\n",
        "print(f\"  ... (총 {len(prompts)}개)\")\n",
        "print(f\">> 총 처리 시간: {hf_batch_time:.3f}초\")\n",
        "print(f\">> 처리량: {len(prompts)/hf_batch_time:.2f} 요청/초\")\n",
        "\n",
        "print(f\"\\n>> 배치 처리 속도 비교: vLLM이 HuggingFace보다 {hf_batch_time/vllm_batch_time:.1f}배 빠름\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 852,
          "referenced_widgets": [
            "22a5f51ab758441fbee50204a01a8462",
            "b91038fc76034b708f1bd0f48376adf6",
            "01f0a3e1f77a4b2db725b4f5bd270e73",
            "b1c7b61c4ed641c8b0837990bcf5bfb3",
            "e8e1128650754e8f9279f41fb403903a",
            "c9c83eaa26394b518cdec57fb335a9c3",
            "a8450e87485d4a019fa0837c619377bb",
            "37a1a172db2a46e59365f2d34b52d94d",
            "1aa28d17b9144fd89cba8fc8ad66cc12",
            "6e34b79d5daa4a458888c89048041995",
            "6fa786a78c1c49e5b165be878485b42e",
            "d9d3272bf8844bafa268a85cfaa34d20",
            "64415c04568e4d89ba9cbe0f81ffefcd",
            "d83213ec46ec4441820e9844ea69f1c1",
            "773eb6b190f14be48aa7a43057cf4145",
            "2a0e7aff414d4beda81c0fb77ce7fd8a",
            "789ec6bd1868470faa871bbfd56172a9",
            "60ddd9f9fa8a4a54a9c8982117398048",
            "0d5117fe53544760801c976c941be73b",
            "01679dde316944128f01a8d86908cdd2",
            "3af89af20fe143eb9ea35b257771fb86",
            "72fa3f5ffbca4dd49cd381e2930ff2a0"
          ]
        },
        "id": "w7Z6QfxkgUh1",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1759378142753,
          "user_tz": -540,
          "elapsed": 7135,
          "user": {
            "displayName": "Paul Youngsub Lim",
            "userId": "17082658625537161560"
          }
        },
        "outputId": "22bbee12-1a6d-4ece-b0c2-107ae5a06e53"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2. 배치 처리 속도 비교 (Continuous Batching)\n",
            "================================================================================\n",
            "\n",
            "[vLLM 배치 추론] 5개 동시 처리\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Adding requests:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "22a5f51ab758441fbee50204a01a8462"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d9d3272bf8844bafa268a85cfaa34d20"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [1] Artificial intelligence can\n",
            "      ->  do all sorts of cool things.\n",
            "How?\n",
            "I'm a programmer and I th...\n",
            "  [2] Machine learning is\n",
            "      ->  a revolutionary new concept in detecting and learning thing...\n",
            "  [3] Deep learning helps\n",
            "      ->  with image quality\n",
            "\n",
            "The quality of images in the SUI (Sensu...\n",
            "  [4] Neural networks are\n",
            "      ->  the elements of our entire human body that interact with ea...\n",
            "  [5] Python programming\n",
            "      ->  is used for many different programming languages, mostly in...\n",
            "  ... (총 5개)\n",
            ">> 총 처리 시간: 0.321초\n",
            ">> 처리량: 15.57 요청/초\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "[HuggingFace 배치 추론] 5개 순차 처리\n",
            "--------------------------------------------------------------------------------\n",
            "  [1] Artificial intelligence can\n",
            "      ->  be a game changer, says researchers\n",
            "There are thousands of ...\n",
            "  [2] Machine learning is\n",
            "      ->  the foundation of everything. You can't just stop learning ...\n",
            "  [3] Deep learning helps\n",
            "      ->  to better understand your world. It's also a great way to l...\n",
            "  [4] Neural networks are\n",
            "      ->  fundamental to the functioning of the brain, and are essent...\n",
            "  [5] Python programming\n",
            "      ->  is a huge part of this. I would recommend getting out there...\n",
            "  ... (총 5개)\n",
            ">> 총 처리 시간: 6.796초\n",
            ">> 처리량: 0.74 요청/초\n",
            "\n",
            ">> 배치 처리 속도 비교: vLLM이 HuggingFace보다 21.2배 빠름\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"3. 메모리 효율성 비교 (PagedAttention): 긴 시퀀스 생성 시 메모리 사용량\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "test_prompt = \"Write a detailed story about artificial intelligence:\"\n",
        "max_tokens = 2000\n",
        "\n",
        "print(f\"\\n최대 생성 토큰: {max_tokens}개\")\n",
        "print(\"─\" * 60)\n",
        "\n",
        "# vLLM 메모리 측정\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "memory_before_vllm = torch.cuda.memory_allocated() / (1024**2)\n",
        "\n",
        "params = SamplingParams(temperature=0.7, max_tokens=max_tokens)\n",
        "start_vllm = time.time()\n",
        "vllm_output = llm.generate([test_prompt], params)\n",
        "vllm_time = time.time() - start_vllm\n",
        "\n",
        "memory_after_vllm = torch.cuda.memory_allocated() / (1024**2)\n",
        "memory_peak_vllm = torch.cuda.max_memory_allocated() / (1024**2)\n",
        "memory_used_vllm = memory_peak_vllm - memory_before_vllm\n",
        "\n",
        "print(f\"\\nvLLM PagedAttention:\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"  처리 시간: {vllm_time:.3f}초\")\n",
        "print(f\"  추론 전 메모리: {memory_before_vllm:.2f} MB\")\n",
        "print(f\"  최대 메모리: {memory_peak_vllm:.2f} MB\")\n",
        "print(f\"  추가 사용: {memory_used_vllm:.2f} MB\")\n",
        "\n",
        "# HuggingFace 메모리 측정\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "memory_before_hf = torch.cuda.memory_allocated() / (1024**2)\n",
        "\n",
        "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "start_hf = time.time()\n",
        "with torch.no_grad():\n",
        "    hf_output = hf_model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_tokens,\n",
        "        temperature=0.7,\n",
        "        do_sample=True\n",
        "    )\n",
        "hf_time = time.time() - start_hf\n",
        "\n",
        "memory_after_hf = torch.cuda.memory_allocated() / (1024**2)\n",
        "memory_peak_hf = torch.cuda.max_memory_allocated() / (1024**2)\n",
        "memory_used_hf = memory_peak_hf - memory_before_hf\n",
        "\n",
        "print(f\"\\nHuggingFace:\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"  처리 시간: {hf_time:.3f}초\")\n",
        "print(f\"  추론 전 메모리: {memory_before_hf:.2f} MB\")\n",
        "print(f\"  최대 메모리: {memory_peak_hf:.2f} MB\")\n",
        "print(f\"  추가 사용: {memory_used_hf:.2f} MB\")\n",
        "\n",
        "# 비교\n",
        "memory_savings = memory_used_hf - memory_used_vllm\n",
        "memory_savings_pct = (memory_savings / memory_used_hf) * 100\n",
        "speed_ratio = hf_time / vllm_time\n",
        "\n",
        "print(\"\\n\" + \"─\" * 60)\n",
        "print(\"비교 결과:\")\n",
        "print(f\"  메모리 절약: {memory_savings:.2f} MB ({memory_savings_pct:.1f}%)\")\n",
        "print(f\"  속도: vLLM이 {speed_ratio:.1f}배 빠름\")\n",
        "print(\"─\" * 60)\n",
        "\n",
        "\n",
        "print(\"\\n vLLM PagedAttention:\")\n",
        "print(\"  - KV 캐시를 고정 크기 블록(페이지)으로 분할\")\n",
        "print(\"  - 필요할 때마다 블록을 동적으로 할당, 메모리 단편화 최소화 및 효율적 재사용\")\n",
        "\n",
        "print(\"\\n HuggingFace:\")\n",
        "print(\"  - KV 캐시를 연속된 메모리 공간에 사전 할당, 최대 시퀀스 길이만큼 메모리 예약\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669,
          "referenced_widgets": [
            "d65ece9d29b14ae3a317c91d4d6075fc",
            "2f6fd4eb2d104281b838ec3713746715",
            "cbaa55c713874b28bd2cf61746fdd09e",
            "781bea0c18364e24b243b3f5a2bbc8c1",
            "e41eccbdb58248248da0cfb4e70503bd",
            "498d58c609a6487d8a46f53163abf818",
            "3d667468883141c5b666f25a7c12b3a0",
            "da6f7b4551d94a21aed44de12141cd72",
            "07f52e3676464f4fa0b1f7e4a05e5c60",
            "3d0da5a269214436afd29a1d44517865",
            "41fd4599a44e4f7b9c0ca1fa3faef679",
            "e39df9b3acb1414bb7ec18be64520ce8",
            "134ec213824941e394008a94df535284",
            "c3c8fc17f55144e0bbd0783cad012a84",
            "6ab9c7c13e274ae7bdc836e72d08b578",
            "366234fe58ba47f1aeacb4dd81070287",
            "11df3f12b6e84dca86f09044dd64f773",
            "9327d2aa398544da8bea7448a87568e0",
            "197ff25e6e574366b0d0cb14ec918cac",
            "a13be3dced1042bf950c190ae448cf69",
            "4a60312045c84dd1b4d878c2a6865c08",
            "1d00e2bd892e46d1884c5dadb605213c"
          ]
        },
        "id": "_6_xJdnPgUcL",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1759378184612,
          "user_tz": -540,
          "elapsed": 41850,
          "user": {
            "displayName": "Paul Youngsub Lim",
            "userId": "17082658625537161560"
          }
        },
        "outputId": "03848953-83e3-481d-88c8-8fb52ff65065"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3. 메모리 효율성 비교 (PagedAttention): 긴 시퀀스 생성 시 메모리 사용량\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "최대 생성 토큰: 2000개\n",
            "────────────────────────────────────────────────────────────\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d65ece9d29b14ae3a317c91d4d6075fc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e39df9b3acb1414bb7ec18be64520ce8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "vLLM PagedAttention:\n",
            "--------------------------------------------------------------------------------\n",
            "  처리 시간: 2.951초\n",
            "  추론 전 메모리: 13580.13 MB\n",
            "  최대 메모리: 13581.25 MB\n",
            "  추가 사용: 1.12 MB\n",
            "\n",
            "HuggingFace:\n",
            "--------------------------------------------------------------------------------\n",
            "  처리 시간: 37.092초\n",
            "  추론 전 메모리: 13580.13 MB\n",
            "  최대 메모리: 13727.83 MB\n",
            "  추가 사용: 147.70 MB\n",
            "\n",
            "────────────────────────────────────────────────────────────\n",
            "비교 결과:\n",
            "  메모리 절약: 146.58 MB (99.2%)\n",
            "  속도: vLLM이 12.6배 빠름\n",
            "────────────────────────────────────────────────────────────\n",
            "\n",
            " vLLM PagedAttention:\n",
            "  - KV 캐시를 고정 크기 블록(페이지)으로 분할\n",
            "  - 필요할 때마다 블록을 동적으로 할당, 메모리 단편화 최소화 및 효율적 재사용\n",
            "\n",
            " HuggingFace:\n",
            "  - KV 캐시를 연속된 메모리 공간에 사전 할당, 최대 시퀀스 길이만큼 메모리 예약\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"4. 동시 사용자 처리 비교\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "user_queries = [f\"User {i}: Hello, my name is\" for i in range(1, 11)]\n",
        "\n",
        "# vLLM 동시 처리\n",
        "print(f\"\\n[vLLM] 10명의 동시 사용자 요청 처리 (Continuous Batching)\")\n",
        "print(\"-\" * 80)\n",
        "start_vllm_users = time.time()\n",
        "vllm_user_outputs = llm.generate(user_queries, SamplingParams(temperature=0.7, max_tokens=20))\n",
        "vllm_users_time = time.time() - start_vllm_users\n",
        "\n",
        "for i in range(3):\n",
        "    print(f\"  {user_queries[i]}\")\n",
        "    print(f\"    -> {vllm_user_outputs[i].outputs[0].text}\")\n",
        "print(f\"  ... (총 10개 요청)\")\n",
        "print(f\">> 처리 시간: {vllm_users_time:.3f}초\")\n",
        "print(f\">> 처리 능력: {10/vllm_users_time:.1f} 사용자/초\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# HuggingFace 순차 처리\n",
        "print(f\"\\n[HuggingFace] 10명의 사용자 요청 순차 처리\")\n",
        "print(\"-\" * 80)\n",
        "start_hf_users = time.time()\n",
        "hf_user_outputs = []\n",
        "for query in user_queries:\n",
        "    inputs = tokenizer(query, return_tensors=\"pt\").to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        outputs = hf_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=20,\n",
        "            temperature=0.7,\n",
        "            do_sample=True\n",
        "        )\n",
        "    hf_user_outputs.append(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "hf_users_time = time.time() - start_hf_users\n",
        "\n",
        "for i in range(3):\n",
        "    print(f\"  {user_queries[i]}\")\n",
        "    print(f\"    -> {hf_user_outputs[i][len(user_queries[i]):]}\")\n",
        "print(f\"  ... (총 10개 요청)\")\n",
        "print(f\">> 처리 시간: {hf_users_time:.3f}초\")\n",
        "print(f\">> 처리 능력: {10/hf_users_time:.1f} 사용자/초\")\n",
        "\n",
        "print(f\"\\n동시 사용자 처리: vLLM이 HuggingFace보다 {hf_users_time/vllm_users_time:.1f}배 빠름\")\n",
        "print(\"\\nvLLM의 Continuous Batching:\")\n",
        "print(\"  - 여러 사용자 요청을 효율적으로 동시 처리, 실시간 서비스에 최적화, 높은 Throughput\")\n",
        "print(\"\\nHuggingFace:\")\n",
        "print(\"  - 기본적으로 순차 처리, 배치 처리도 가능하지만 vLLM만큼 최적화되지 않음\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 724,
          "referenced_widgets": [
            "116d06c43f794a189765195dbf1cacef",
            "c9cd9b7346a840c59e00fc5aafc0015f",
            "c4e0737225744223950bd827a80e720f",
            "d23628bbb6974bf48c4a3c2e55758ceb",
            "eae6c1523e164acca3fe3cc0cad20c76",
            "bbb573559d5543a491b9c3f54f71b277",
            "bfbf14176b474bf6bf035c96dfda683e",
            "f995e9fb9b7040eba15c8b63cc4f3951",
            "4746a9ac02664f779bfc42693c295132",
            "ec363931d8f74e1b97677904a06b66f3",
            "e9cfb33cdaac4d008e2f371189d17326",
            "eca8d60154774a59ba85da7bac98fc7b",
            "c897d77f1e0241c9a99fc11caf77e90c",
            "4dea5b1277db4a368d5786d51a960e94",
            "030a0fe1632d4f2398e7b0043506725d",
            "b5a19f0004d248739fdbe5c6cbbc3724",
            "311f52ced1c944fa913c524d5f883d0d",
            "6a56d7caf972493a9facd8498be7981a",
            "3c884ea1a11e4b19ab5c92cbde88c2ac",
            "e7157eb94f8649999226b99adcb3530b",
            "427fd992be694b5bbaff225c5a29baf6",
            "6d1b27818d6045a791d6664f9cfd9f2a"
          ]
        },
        "id": "zXWh9b8HgUZW",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1759378186436,
          "user_tz": -540,
          "elapsed": 1822,
          "user": {
            "displayName": "Paul Youngsub Lim",
            "userId": "17082658625537161560"
          }
        },
        "outputId": "b75efdb5-b052-4c20-ca0f-81944d7fd6cb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4. 동시 사용자 처리 비교\n",
            "================================================================================\n",
            "\n",
            "[vLLM] 10명의 동시 사용자 요청 처리 (Continuous Batching)\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "116d06c43f794a189765195dbf1cacef"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eca8d60154774a59ba85da7bac98fc7b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  User 1: Hello, my name is\n",
            "    ->  Riki. I'm in the middle of a rapitation, and I'm going to start using\n",
            "  User 2: Hello, my name is\n",
            "    ->  Ben. I am a 22 year old male. I am currently enjoying the best nights of my life\n",
            "  User 3: Hello, my name is\n",
            "    ->  Brandon and I am very new to this site. I am a bartender, I'm trying to get\n",
            "  ... (총 10개 요청)\n",
            ">> 처리 시간: 0.130초\n",
            ">> 처리 능력: 76.8 사용자/초\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "[HuggingFace] 10명의 사용자 요청 순차 처리\n",
            "--------------------------------------------------------------------------------\n",
            "  User 1: Hello, my name is\n",
            "    ->  Tom. I'm a new person. I'm a very nice guy. I'm a really good\n",
            "  User 2: Hello, my name is\n",
            "    ->  John and I am currently a graduate student at the University of Florida. I'm a member of the\n",
            "  User 3: Hello, my name is\n",
            "    ->  Jaden and I have a friend who I’ve been dating for about a year now.\n",
            "  ... (총 10개 요청)\n",
            ">> 처리 시간: 1.739초\n",
            ">> 처리 능력: 5.8 사용자/초\n",
            "\n",
            "동시 사용자 처리: vLLM이 HuggingFace보다 13.3배 빠름\n",
            "\n",
            "vLLM의 Continuous Batching:\n",
            "  - 여러 사용자 요청을 효율적으로 동시 처리, 실시간 서비스에 최적화, 높은 Throughput\n",
            "\n",
            "HuggingFace:\n",
            "  - 기본적으로 순차 처리, 배치 처리도 가능하지만 vLLM만큼 최적화되지 않음\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KfuTc-ilKBJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vpHIDXGBgUQ6",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1759378186488,
          "user_tz": -540,
          "elapsed": 50,
          "user": {
            "displayName": "Paul Youngsub Lim",
            "userId": "17082658625537161560"
          }
        }
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WI3kf8tCgULL",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1759378186490,
          "user_tz": -540,
          "elapsed": 4,
          "user": {
            "displayName": "Paul Youngsub Lim",
            "userId": "17082658625537161560"
          }
        }
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}