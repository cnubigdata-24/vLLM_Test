{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6c25bbbd795a48a5b27aada40711579a",
            "49fa2d8436a5499c8999ad89baabb1c8",
            "81b6807e739a43b4a2918a5a08fa80a7",
            "99b5432b853b480e99634986690fb93b",
            "d0d23ab2fec541d5950a6e4853e535b2",
            "155b100844b74738b6b02b4e86a48695",
            "cfff1877de4247e2a7479e15143d47c9",
            "0b106d3c7e38411cb80f864e9f29f5fc",
            "fe75e9d686b24af692d4e1bab571f77a",
            "d240ab0a14d143f299ce2174f0b8cd20",
            "3430776df62a4981ae9b1f9a4aa4d13f",
            "9fffedca769a4c138b007cdc02e2bae7",
            "f2afd5413ca0495ba09f6d213325acc1",
            "c8e9bc9faefd4b1ca3772bfe9a7989e2",
            "e38d37a0497748018fad1b81845831f9",
            "1f70721950bb40eb85d11b0b81cc6a7a",
            "fe7657d79ff343c697f2815c5331692b",
            "9692c25b4adf4b389ec76c552de357df",
            "73f0698938d84e2fa8c4028dc5d09e62",
            "beb69ef47e234f5faa546d8a89c6cc20",
            "8e3bd7a40ffd46c19bc4a1e3e4c39712",
            "7d038fd03e774eea9d6dd22d3ca3b92a",
            "e9fb83621f3b4635877a994a07990214",
            "62d03bc527734a79bcfdeddd8ca1d21d",
            "5d958b84ccad4bb28a2f17fe58738686",
            "c33274cf64cf413185e0a65fe65ba6f0",
            "ebd3903b2cbb4173823a47bb4835656b",
            "be1fe4575ab74bb385144a11cf8b0909",
            "5cd07107013348988459d51605ab2963",
            "083512b6f8f046d29e0f742d245c4318",
            "3dabe26ed23f4bf0a8bb2ec07cc63d43",
            "df3d15d026a0463c8e33e089858ce193",
            "040bb5ad8c5b49ceac1ce44849784352",
            "2be87e3d3a4f4478ba112ab18986e7f2",
            "957822ce3b0d4c4d88cdfe80626adfcc",
            "f452355730914b66870024b4808f49e9",
            "d8b3b245c2f3409f82bbc070fe8f7516",
            "9958afdc915b47dd9578789467ede47f",
            "248f10044c384b019c80e3424d7ef8b6",
            "075f52eea1464c82b5248e6740108a6f",
            "cafb7396c74d497fb5007fd0ec6646d8",
            "e7463fde140c46038acc75d95843f2dc",
            "948cd0aee44940bcb00a03f6d88fc252",
            "380f8f3e7b004735841ae33406eb1049",
            "5d0fe08369d44b36914f9794ac54bd52",
            "f826cb0d439a49e1960569380f9100f8",
            "a0a1160bfc3a4db78ea2d16f1da2d822",
            "e207f1719f584bf680f907a996abeb54",
            "26297b1d4af44a1b88bb8993f11b76b5",
            "3e44fc5c9e9d4fccabf54ea37e616ba1",
            "f59320e0fa5a41d7aa5cd750122416b6",
            "5e302e5e9dc247d3a143f16ff7676dc4",
            "a83696f720024a2abfb1441fa20da31b",
            "494c9b9bf73a44e19c3a2c19b07c6de0",
            "881efd01a8e94c7096d7f45811eeb8b9",
            "4cbf506a7add41c48898fb1893415e08",
            "2772f5100b244f95b7220663273a0b29",
            "d6326938eac84c29b38890380fb69e1c",
            "44198085f00d40e7b5750005bff22c70",
            "0db6cff33787426883751e5f9221aacf",
            "7c7c1e8995974642b5068d002f1a5fde",
            "3d48b22f89414446b2a534d5d664f71a",
            "82dd23777f254d529eefe0db558fb2b5",
            "748d9917b2f34efdb8f743d9d2a8b9d7",
            "7858fecf97c4488b8da2068532a6eaa4",
            "369dd5677bb9409db2a57fe2be2a26c7",
            "9f8048513e81419f9e04ded723dcfd8f",
            "92055a08b6e64825b302c861ab6c9e52",
            "2b11c03299804ece925a07d125b5c7ed",
            "46bb1103696f48b9834353b3a6ddfd3c",
            "4abf74b122304f708c80b7ff9bffc3fc",
            "7e3720a9d32246ae866283d0df636865",
            "f917b5941cdf4f0baadd217024a9d1a8",
            "2ba73b361e9a4c508ca1de5b8c855d10",
            "36f4bf1466834a459a1a89e06a250af7",
            "8a5b16feea20480fba5ac0ff20afb64b",
            "4cd2792bb4b54da59fbd4a2c21887909",
            "5acb4cbf80af4314a08ce7d5e8d73f09",
            "520b4dbc0f1e4a519e36b37a0b79f935",
            "5dd99c59ab2344c9b84db0fab64b5e98",
            "22a742aaa6804a5f84df202664250666",
            "897dbfe53a41437a9cc2281c13375f8f",
            "79dd04fb5b254534a3e4fc96a5a27328",
            "1ad44982e6b246eaa9c41d345a533198",
            "2577aac88b0741678aaa8fc22abff274",
            "e9393d4cbdd14ffc862879fbce2418bc",
            "20fc48214a1e43c9b08cafe25a69c59e",
            "4c3c76e7b807406f8d37cd6de9e1aae0",
            "b9de8465fb6248738411170daf5f2073",
            "abd3607a53774fb68fa8ae95f34f5f11",
            "2c262a633e014f4e93ca9ac5569b1e9c",
            "691d103900a04ada8146e003ab6a32fa",
            "16a5d79046e34349813514ebeb60d83f",
            "cedc22e2c3fb4d8f99a584324dfbe93c",
            "c49954e6841d45d982109ffc1e3a9701",
            "d58540b3221a4d4a92154baacf8362d6",
            "08f88ce48b764f93b98671988717d829",
            "a66ac21857b34a2cad30a1873c9dc737",
            "815622f16601471f81d6b821fb9eefc3",
            "8397728ead0c41af87eeae14a7de330f",
            "e06cbf7518b2473ab9f59a203474d6e7",
            "a76af0d5cd6547afb9c66c3d5b0fbc10",
            "8d5a82fe3a614812bc2431aa66cc9c66",
            "7b02d73a0e2e4d879c201214849aca45",
            "2589571c83d04e0ebab66950dd2cfc2c",
            "7fa31b99d8c54e89aff649b9b68adb10",
            "834aba01941a4b9798c18f6811d23da6",
            "e000a80a37c44a858fe2610a44db1aa5",
            "e057f707663b4340b389b90531aac984",
            "ba01a3e7224f4350b843618a57b03197",
            "22eb6be713fc452b85e560457ae08a69",
            "457c31f4d69a46a9abc289fd51b790d3",
            "e3a633ff374b4ef6898dd349ef814c19",
            "ba02805420074943953fd4a1b99f8447",
            "0044c63ec2314ad1af5458fe46aa9dd4",
            "f04fb07dabea4bf9a72596fa40720bcd",
            "7e0daae8a2844b2182b8b64e4273d77b",
            "bb757b4361af446faf7ef7c1828badc3",
            "28e39bbf2acb4bb6bd535daccb130365",
            "a3c772c8a98a4a78bd387d0d42de9e84",
            "6fac9d09bfb043db86e91e3213f64f5d",
            "622402989fba44f7896d9f52d2c2edb8",
            "9d4e6b7c09f1498f82b6fe97bed22e03",
            "b4cfe04cc02c487b9d2f1533c3cb6872",
            "682e713579bb48d182cf88dc564df64b",
            "560bf5f49b9c4bacbb40ad67aa3ce9eb",
            "307fd839eb6749378ac7b9f64bc3493a",
            "f482dc46812f44058669b7eea8809fa7",
            "358c161a6449452681039e6358274a62",
            "c76a42a64d58484ebcd1096a996073a2",
            "e1f21daf98de4cf8b17c8d66611a77bc",
            "672b26a6681348979db828a2d9e3ff99",
            "3969c4559014400d9d79fcbb709c3418",
            "438a97123882449eb5a0b30e4c0097eb",
            "117993ee6d114d7e92de8f029e8ac395",
            "3f882b104db64b91bb27223a7f48036a",
            "15246cfe88ea4f77bf1be4c763224b42",
            "dbb758e33c434c97b94760a8acc355dd",
            "3de5249029a84793a8fa17107538714e",
            "c683c81706af4aaaa7c6b0ae26f66c3b",
            "080d706691c448fdb8c9fd924b672f15",
            "6d384f1207834e938fca02f4e983e17e",
            "147fdc49b2f5463890d0b20a1f848450",
            "ad5b1715d1cf4297ab56132c94030f1e",
            "0ebde996493841b1a03012e8c24ecc08",
            "a734f4826df449129cda35819db98e86",
            "7cd3b542c5e84bc484e532659c2ea378",
            "425b46fde1074a499fd85b6cb48aa0ba",
            "911720e2080643648f48057cd91de86e",
            "7c881a2f99184a8e9c5e1504a3e39343",
            "c6efdc8223c04a5b8458f642a09573cc",
            "76d3d988de2841e5a9437ead67dee921",
            "b53ef6c531a141389f2593f26a05d401",
            "5acdc520c8c847fcb9a794e3f2faf63e",
            "24757e726cc44cd0867b58f10bf48b08",
            "634799fc79724d1b824df8b8763caa80",
            "5cf0f6732a1246238bb01e01e6aa78ec",
            "6b0c5b5c0eb74f20a2577721aad31ba9",
            "d524dfc9044649c8a771cc562a12e64b",
            "c0ef8db2b81d479890141baebf56ecb2",
            "67e1b3b5481743758827f3f449fa153f",
            "2c14c04a431c4eeaa5eb248bc3f4e144",
            "7e76972b76be4e8685e9438b444ca695",
            "0aad0d54f5c04bc18d53972e1c172c51",
            "b50e8e85ecab4b558fb97f3994577ee2"
          ]
        },
        "id": "PUw0cbJ_cNGt",
        "outputId": "416cff72-eaa0-46a6-8e01-8b1d77073e7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Nov  6 13:57:53 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "================================================================================\n",
            "vLLM 설치 시작...\n",
            "================================================================================\n",
            "pyairports 설치 중...\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyairports (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "vLLM 설치 중...\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.2/438.2 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.0/180.0 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m396.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m501.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m395.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.2/117.2 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m112.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.0/15.0 MB\u001b[0m \u001b[31m121.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m133.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m114.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.0/388.0 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m285.7/285.7 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.6/213.6 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m138.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m124.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m959.8/959.8 kB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "================================================================================\n",
            "설치 완료! 런타임 재시작 중...\n",
            "================================================================================\n",
            "================================================================================\n",
            "테스트 시작\n",
            "================================================================================\n",
            "INFO 11-06 13:59:48 [__init__.py:216] Automatically detected platform cuda.\n",
            "\n",
            "[모델 로딩 시작] facebook/opt-125m\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "1. vLLM 모델 로드... \n",
            "--------------------------------------------------------------------------------\n",
            "INFO 11-06 14:00:01 [utils.py:233] non-default args: {'disable_log_stats': True, 'model': 'facebook/opt-125m'}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/651 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6c25bbbd795a48a5b27aada40711579a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 11-06 14:00:23 [model.py:547] Resolved architecture: OPTForCausalLM\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 11-06 14:00:23 [model.py:1510] Using max model len 2048\n",
            "INFO 11-06 14:00:26 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=8192.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/685 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9fffedca769a4c138b007cdc02e2bae7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e9fb83621f3b4635877a994a07990214"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2be87e3d3a4f4478ba112ab18986e7f2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/441 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5d0fe08369d44b36914f9794ac54bd52"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4cbf506a7add41c48898fb1893415e08"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 11-06 14:00:29 [__init__.py:3036] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
            "INFO 11-06 14:01:11 [llm.py:306] Supported_tasks: ['generate']\n",
            "   vLLM 로드 시간: 70.13초\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "2. HuggingFace 모델 로드 중...\n",
            "--------------------------------------------------------------------------------\n",
            "   HuggingFace 로드 시간: 3.51초\n",
            "--------------------------------------------------------------------------------\n",
            ">> vLLM의 초기 로드 시간이 HuggingFace보다 긴 것이 일반적:  KV 캐시 블록 풀 메모리 사전 할당, CUDA 커널들을 사전 컴파일하고 최적화, 배치 처리 준비\n",
            ">> HuggingFace는 모델만 로드하고 추론 시점에 필요한 만큼만 메모리를 할당하여 초기 로딩 속도가 더 빠름\n",
            "\n",
            "1. 단일 추론 속도 비교\n",
            "================================================================================\n",
            "\n",
            "[vLLM 추론]\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9f8048513e81419f9e04ded723dcfd8f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5acb4cbf80af4314a08ce7d5e8d73f09"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/251M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b9de8465fb6248738411170daf5f2073"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> 입력: The future of AI is\n",
            ">> 출력:  cloud computing\n",
            "\n",
            "Jose Antonia de Armasis/Getty Images\n",
            "\n",
            "As Microsoft Office is set to become the nex...\n",
            ">> 처리 시간: 11.275초\n",
            "\n",
            "[HuggingFace 추론]\n",
            "--------------------------------------------------------------------------------\n",
            ">> 입력: The future of AI is\n",
            ">> 출력:  now; The future is now\n",
            "A new phase in the development of AI is emerging. The AI revolution that sta...\n",
            ">> 처리 시간: 1.361초\n",
            "\n",
            ">> 속도 비교: vLLM이 HuggingFace보다 0.1배 빠름\n",
            "\n",
            "2. 배치 처리 속도 비교 (Continuous Batching)\n",
            "================================================================================\n",
            "\n",
            "[vLLM 배치 추론] 5개 동시 처리\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Adding requests:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8397728ead0c41af87eeae14a7de330f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "22eb6be713fc452b85e560457ae08a69"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [1] Artificial intelligence can\n",
            "      ->  be used in a variety of ways. For example, artificial intel...\n",
            "  [2] Machine learning is\n",
            "      ->  a self-evolution. I think it's more likely that there's a c...\n",
            "  [3] Deep learning helps\n",
            "      ->  with genetics, so I'd recommend learning it in your home sc...\n",
            "  [4] Neural networks are\n",
            "      ->  a powerful media medium, which makes them a powerful way of...\n",
            "  [5] Python programming\n",
            "      ->  language VB.NET\n",
            "\n",
            "VB.NET is a programming language for progr...\n",
            "  ... (총 5개)\n",
            ">> 총 처리 시간: 14.438초\n",
            ">> 처리량: 0.35 요청/초\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "[HuggingFace 배치 추론] 5개 순차 처리\n",
            "--------------------------------------------------------------------------------\n",
            "  [1] Artificial intelligence can\n",
            "      ->  take advantage of this for a lot of reasons: it's useful fo...\n",
            "  [2] Machine learning is\n",
            "      ->  the ability to make a video that is both interesting and in...\n",
            "  [3] Deep learning helps\n",
            "      ->  predict what the virus will look like\n",
            "Scientists at the Uni...\n",
            "  [4] Neural networks are\n",
            "      ->  a core part of networks. The network consists of a pluralit...\n",
            "  [5] Python programming\n",
            "      ->  is my favorite thing to learn. My biggest problem with it i...\n",
            "  ... (총 5개)\n",
            ">> 총 처리 시간: 2.101초\n",
            ">> 처리량: 2.38 요청/초\n",
            "\n",
            ">> 배치 처리 속도 비교: vLLM이 HuggingFace보다 0.1배 빠름\n",
            "\n",
            "3. 메모리 효율성 비교 (PagedAttention): 긴 시퀀스 생성 시 메모리 사용량\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "최대 생성 토큰: 2000개\n",
            "────────────────────────────────────────────────────────────\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "622402989fba44f7896d9f52d2c2edb8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3969c4559014400d9d79fcbb709c3418"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "vLLM PagedAttention:\n",
            "--------------------------------------------------------------------------------\n",
            "  처리 시간: 2.376초\n",
            "  추론 전 메모리: 487.09 MB\n",
            "  최대 메모리: 487.09 MB\n",
            "  추가 사용: 0.00 MB\n",
            "\n",
            "HuggingFace:\n",
            "--------------------------------------------------------------------------------\n",
            "  처리 시간: 35.190초\n",
            "  추론 전 메모리: 487.09 MB\n",
            "  최대 메모리: 635.41 MB\n",
            "  추가 사용: 148.32 MB\n",
            "\n",
            "────────────────────────────────────────────────────────────\n",
            "비교 결과:\n",
            "  메모리 절약: 148.32 MB (100.0%)\n",
            "  속도: vLLM이 14.8배 빠름\n",
            "────────────────────────────────────────────────────────────\n",
            "\n",
            " vLLM PagedAttention:\n",
            "  - KV 캐시를 고정 크기 블록(페이지)으로 분할\n",
            "  - 필요할 때마다 블록을 동적으로 할당, 메모리 단편화 최소화 및 효율적 재사용\n",
            "\n",
            " HuggingFace:\n",
            "  - KV 캐시를 연속된 메모리 공간에 사전 할당, 최대 시퀀스 길이만큼 메모리 예약\n",
            "\n",
            "4. 동시 사용자 처리 비교\n",
            "================================================================================\n",
            "\n",
            "[vLLM] 10명의 동시 사용자 요청 처리 (Continuous Batching)\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ad5b1715d1cf4297ab56132c94030f1e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "24757e726cc44cd0867b58f10bf48b08"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  User 1: Hello, my name is\n",
            "    ->  Salvador, my name is Liliana, i was born in Mexico, my dad was a singer from\n",
            "  User 2: Hello, my name is\n",
            "    ->  JT and I am a Skrill user. I use Skrill for searching and typing, because\n",
            "  User 3: Hello, my name is\n",
            "    ->  Elena, and I am a computer science project graduate student. I will be teaching computer science at the\n",
            "  ... (총 10개 요청)\n",
            ">> 처리 시간: 0.749초\n",
            ">> 처리 능력: 13.4 사용자/초\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "[HuggingFace] 10명의 사용자 요청 순차 처리\n",
            "--------------------------------------------------------------------------------\n",
            "  User 1: Hello, my name is\n",
            "    ->  Gengi and I am new to all this. I am a fan of the anime \"The\n",
            "  User 2: Hello, my name is\n",
            "    ->  Sam, and I am the author of my first book.\n",
            "\n",
            "I am a professional writer,\n",
            "  User 3: Hello, my name is\n",
            "    ->  Jai.\n",
            "\n",
            "Hello, my name is Jai.\n",
            "\n",
            "I am a student.\n",
            "  ... (총 10개 요청)\n",
            ">> 처리 시간: 4.117초\n",
            ">> 처리 능력: 2.4 사용자/초\n",
            "\n",
            "동시 사용자 처리: vLLM이 HuggingFace보다 5.5배 빠름\n",
            "\n",
            "vLLM의 Continuous Batching:\n",
            "  - 여러 사용자 요청을 효율적으로 동시 처리, 실시간 서비스에 최적화, 높은 Throughput\n",
            "\n",
            "HuggingFace:\n",
            "  - 기본적으로 순차 처리, 배치 처리도 가능하지만 vLLM만큼 최적화되지 않음\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# GPU 확인\n",
        "get_ipython().system('nvidia-smi')\n",
        "\n",
        "# vLLM 설치 여부 확인\n",
        "try:\n",
        "    import vllm\n",
        "    VLLM_INSTALLED = True\n",
        "except ImportError:\n",
        "    VLLM_INSTALLED = False\n",
        "\n",
        "if not VLLM_INSTALLED:\n",
        "    print(\"=\"*80)\n",
        "    print(\"vLLM 설치 시작...\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(\"pyairports 설치 중...\")\n",
        "    get_ipython().system('pip install pyairports -q')\n",
        "    print(\"vLLM 설치 중...\")\n",
        "    get_ipython().system('pip install vllm -q')\n",
        "\n",
        "    print(\"\\n\"+\"=\"*80)\n",
        "    print(\"설치 완료! 런타임 재시작 중...\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    import IPython\n",
        "    IPython.Application.instance().kernel.do_shutdown(True)\n",
        "\n",
        "# 여기부터는 vLLM이 설치되어 있을 때만 실행됨\n",
        "print(\"=\"*80)\n",
        "print(\"테스트 시작\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "os.environ['VLLM_USE_V1'] = '1'\n",
        "\n",
        "import torch\n",
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "from vllm import LLM, SamplingParams\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import time\n",
        "\n",
        "model_name = \"facebook/opt-125m\"\n",
        "print(f\"\\n[모델 로딩 시작] {model_name}\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "print(\"\\n1. vLLM 모델 로드... \")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "start_vllm_load = time.time()\n",
        "llm = LLM(model=model_name)\n",
        "vllm_load_time = time.time() - start_vllm_load\n",
        "\n",
        "print(f\"   vLLM 로드 시간: {vllm_load_time:.2f}초\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "print(\"\\n2. HuggingFace 모델 로드 중...\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "start_hf_load = time.time()\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "hf_model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\")\n",
        "hf_load_time = time.time() - start_hf_load\n",
        "\n",
        "print(f\"   HuggingFace 로드 시간: {hf_load_time:.2f}초\")\n",
        "print(\"-\" * 80)\n",
        "print(f\">> vLLM의 초기 로드 시간이 HuggingFace보다 긴 것이 일반적:  KV 캐시 블록 풀 메모리 사전 할당, CUDA 커널들을 사전 컴파일하고 최적화, 배치 처리 준비\")\n",
        "print(f\">> HuggingFace는 모델만 로드하고 추론 시점에 필요한 만큼만 메모리를 할당하여 초기 로딩 속도가 더 빠름\")\n",
        "\n",
        "print(\"\\n1. 단일 추론 속도 비교\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "prompt = \"The future of AI is\"\n",
        "params = SamplingParams(temperature=0.8, max_tokens=50)\n",
        "\n",
        "# vLLM\n",
        "print(\"\\n[vLLM 추론]\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "start_vllm = time.time()\n",
        "vllm_outputs = llm.generate([prompt], params)\n",
        "vllm_time = time.time() - start_vllm\n",
        "\n",
        "print(f\">> 입력: {prompt}\")\n",
        "print(f\">> 출력: {vllm_outputs[0].outputs[0].text[:100]}...\")\n",
        "print(f\">> 처리 시간: {vllm_time:.3f}초\")\n",
        "\n",
        "# HuggingFace\n",
        "print(\"\\n[HuggingFace 추론]\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "start_hf = time.time()\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "with torch.no_grad():\n",
        "    hf_outputs = hf_model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=50,\n",
        "        temperature=0.8,\n",
        "        do_sample=True\n",
        "    )\n",
        "hf_time = time.time() - start_hf\n",
        "hf_text = tokenizer.decode(hf_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(f\">> 입력: {prompt}\")\n",
        "print(f\">> 출력: {hf_text[len(prompt):len(prompt)+100]}...\")\n",
        "print(f\">> 처리 시간: {hf_time:.3f}초\")\n",
        "\n",
        "print(f\"\\n>> 속도 비교: vLLM이 HuggingFace보다 {hf_time/vllm_time:.1f}배 빠름\")\n",
        "\n",
        "print(\"\\n2. 배치 처리 속도 비교 (Continuous Batching)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "prompts = [\n",
        "    \"Artificial intelligence can\",\n",
        "    \"Machine learning is\",\n",
        "    \"Deep learning helps\",\n",
        "    \"Neural networks are\",\n",
        "    \"Python programming\"\n",
        "]\n",
        "\n",
        "# vLLM\n",
        "print(f\"\\n[vLLM 배치 추론] {len(prompts)}개 동시 처리\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "start_vllm_batch = time.time()\n",
        "vllm_batch_outputs = llm.generate(prompts, params)\n",
        "vllm_batch_time = time.time() - start_vllm_batch\n",
        "\n",
        "for i, output in enumerate(vllm_batch_outputs[:5]):\n",
        "    print(f\"  [{i+1}] {prompts[i]}\")\n",
        "    print(f\"      -> {output.outputs[0].text[:60]}...\")\n",
        "\n",
        "print(f\"  ... (총 {len(prompts)}개)\")\n",
        "print(f\">> 총 처리 시간: {vllm_batch_time:.3f}초\")\n",
        "print(f\">> 처리량: {len(prompts)/vllm_batch_time:.2f} 요청/초\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# HuggingFace\n",
        "print(f\"\\n[HuggingFace 배치 추론] {len(prompts)}개 순차 처리\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "start_hf_batch = time.time()\n",
        "hf_batch_outputs = []\n",
        "for prompt_item in prompts:\n",
        "    inputs = tokenizer(prompt_item, return_tensors=\"pt\").to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        outputs = hf_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=50,\n",
        "            temperature=0.8,\n",
        "            do_sample=True\n",
        "        )\n",
        "    hf_batch_outputs.append(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "hf_batch_time = time.time() - start_hf_batch\n",
        "\n",
        "for i in range(5):\n",
        "    print(f\"  [{i+1}] {prompts[i]}\")\n",
        "    print(f\"      -> {hf_batch_outputs[i][len(prompts[i]):len(prompts[i])+60]}...\")\n",
        "\n",
        "print(f\"  ... (총 {len(prompts)}개)\")\n",
        "print(f\">> 총 처리 시간: {hf_batch_time:.3f}초\")\n",
        "print(f\">> 처리량: {len(prompts)/hf_batch_time:.2f} 요청/초\")\n",
        "\n",
        "print(f\"\\n>> 배치 처리 속도 비교: vLLM이 HuggingFace보다 {hf_batch_time/vllm_batch_time:.1f}배 빠름\")\n",
        "\n",
        "print(\"\\n3. 메모리 효율성 비교 (PagedAttention): 긴 시퀀스 생성 시 메모리 사용량\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "test_prompt = \"Write a detailed story about artificial intelligence:\"\n",
        "max_tokens = 2000\n",
        "\n",
        "print(f\"\\n최대 생성 토큰: {max_tokens}개\")\n",
        "print(\"─\" * 60)\n",
        "\n",
        "# vLLM 메모리 측정\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "memory_before_vllm = torch.cuda.memory_allocated() / (1024**2)\n",
        "\n",
        "params = SamplingParams(temperature=0.7, max_tokens=max_tokens)\n",
        "start_vllm = time.time()\n",
        "vllm_output = llm.generate([test_prompt], params)\n",
        "vllm_time = time.time() - start_vllm\n",
        "\n",
        "memory_after_vllm = torch.cuda.memory_allocated() / (1024**2)\n",
        "memory_peak_vllm = torch.cuda.max_memory_allocated() / (1024**2)\n",
        "memory_used_vllm = memory_peak_vllm - memory_before_vllm\n",
        "\n",
        "print(f\"\\nvLLM PagedAttention:\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"  처리 시간: {vllm_time:.3f}초\")\n",
        "print(f\"  추론 전 메모리: {memory_before_vllm:.2f} MB\")\n",
        "print(f\"  최대 메모리: {memory_peak_vllm:.2f} MB\")\n",
        "print(f\"  추가 사용: {memory_used_vllm:.2f} MB\")\n",
        "\n",
        "# HuggingFace 메모리 측정\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "memory_before_hf = torch.cuda.memory_allocated() / (1024**2)\n",
        "\n",
        "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "start_hf = time.time()\n",
        "with torch.no_grad():\n",
        "    hf_output = hf_model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_tokens,\n",
        "        temperature=0.7,\n",
        "        do_sample=True\n",
        "    )\n",
        "hf_time = time.time() - start_hf\n",
        "\n",
        "memory_after_hf = torch.cuda.memory_allocated() / (1024**2)\n",
        "memory_peak_hf = torch.cuda.max_memory_allocated() / (1024**2)\n",
        "memory_used_hf = memory_peak_hf - memory_before_hf\n",
        "\n",
        "print(f\"\\nHuggingFace:\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"  처리 시간: {hf_time:.3f}초\")\n",
        "print(f\"  추론 전 메모리: {memory_before_hf:.2f} MB\")\n",
        "print(f\"  최대 메모리: {memory_peak_hf:.2f} MB\")\n",
        "print(f\"  추가 사용: {memory_used_hf:.2f} MB\")\n",
        "\n",
        "# 비교\n",
        "memory_savings = memory_used_hf - memory_used_vllm\n",
        "memory_savings_pct = (memory_savings / memory_used_hf) * 100\n",
        "speed_ratio = hf_time / vllm_time\n",
        "\n",
        "print(\"\\n\" + \"─\" * 60)\n",
        "print(\"비교 결과:\")\n",
        "print(f\"  메모리 절약: {memory_savings:.2f} MB ({memory_savings_pct:.1f}%)\")\n",
        "print(f\"  속도: vLLM이 {speed_ratio:.1f}배 빠름\")\n",
        "print(\"─\" * 60)\n",
        "\n",
        "print(\"\\n vLLM PagedAttention:\")\n",
        "print(\"  - KV 캐시를 고정 크기 블록(페이지)으로 분할\")\n",
        "print(\"  - 필요할 때마다 블록을 동적으로 할당, 메모리 단편화 최소화 및 효율적 재사용\")\n",
        "\n",
        "print(\"\\n HuggingFace:\")\n",
        "print(\"  - KV 캐시를 연속된 메모리 공간에 사전 할당, 최대 시퀀스 길이만큼 메모리 예약\")\n",
        "\n",
        "print(\"\\n4. 동시 사용자 처리 비교\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "user_queries = [f\"User {i}: Hello, my name is\" for i in range(1, 11)]\n",
        "\n",
        "# vLLM 동시 처리\n",
        "print(f\"\\n[vLLM] 10명의 동시 사용자 요청 처리 (Continuous Batching)\")\n",
        "print(\"-\" * 80)\n",
        "start_vllm_users = time.time()\n",
        "vllm_user_outputs = llm.generate(user_queries, SamplingParams(temperature=0.7, max_tokens=20))\n",
        "vllm_users_time = time.time() - start_vllm_users\n",
        "\n",
        "for i in range(3):\n",
        "    print(f\"  {user_queries[i]}\")\n",
        "    print(f\"    -> {vllm_user_outputs[i].outputs[0].text}\")\n",
        "print(f\"  ... (총 10개 요청)\")\n",
        "print(f\">> 처리 시간: {vllm_users_time:.3f}초\")\n",
        "print(f\">> 처리 능력: {10/vllm_users_time:.1f} 사용자/초\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# HuggingFace 순차 처리\n",
        "print(f\"\\n[HuggingFace] 10명의 사용자 요청 순차 처리\")\n",
        "print(\"-\" * 80)\n",
        "start_hf_users = time.time()\n",
        "hf_user_outputs = []\n",
        "for query in user_queries:\n",
        "    inputs = tokenizer(query, return_tensors=\"pt\").to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        outputs = hf_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=20,\n",
        "            temperature=0.7,\n",
        "            do_sample=True\n",
        "        )\n",
        "    hf_user_outputs.append(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "hf_users_time = time.time() - start_hf_users\n",
        "\n",
        "for i in range(3):\n",
        "    print(f\"  {user_queries[i]}\")\n",
        "    print(f\"    -> {hf_user_outputs[i][len(user_queries[i]):]}\")\n",
        "print(f\"  ... (총 10개 요청)\")\n",
        "print(f\">> 처리 시간: {hf_users_time:.3f}초\")\n",
        "print(f\">> 처리 능력: {10/hf_users_time:.1f} 사용자/초\")\n",
        "\n",
        "print(f\"\\n동시 사용자 처리: vLLM이 HuggingFace보다 {hf_users_time/vllm_users_time:.1f}배 빠름\")\n",
        "print(\"\\nvLLM의 Continuous Batching:\")\n",
        "print(\"  - 여러 사용자 요청을 효율적으로 동시 처리, 실시간 서비스에 최적화, 높은 Throughput\")\n",
        "print(\"\\nHuggingFace:\")\n",
        "print(\"  - 기본적으로 순차 처리, 배치 처리도 가능하지만 vLLM만큼 최적화되지 않음\")"
      ]
    }
  ]
}